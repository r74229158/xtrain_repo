{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2a307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Training.Data.Modules.custom_loader import CustomLoader \n",
    "from Utils.model_utils import *; \n",
    "from Utils.configs import conf\n",
    "from XAI_Method.rand_samples import RandomSamples\n",
    "from XAI_Method.causal_effect import Causal\n",
    "from XAI_Applications.evaluation import deletion_auc\n",
    "from XAI_Applications.evaluation import benchmark_on_batch\n",
    "from XAI_Applications.average_drop import evaluate_pixel_erasure\n",
    "from Training.Utils.train_utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a948b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    os.makedirs(os.getcwd() + f\"/Results/Experiments/02_breaking_linearity\", exist_ok=True)\n",
    "    \n",
    "    config = conf['simpleDNNTMnist']\n",
    "    def init_model(rand=True):\n",
    "        return get_model_architecture(config['model_name'], config['model_layers'] + [config[\"num_classes\"]], rand)\n",
    "\n",
    "    model = init_model()   \n",
    "    data_loader = CustomLoader(config[\"dataset\"], True, config[\"batch_size\"], shuffle_test=True)\n",
    "    train_loader, test_loader = data_loader.train_load, data_loader.evalu_load\n",
    "\n",
    "    rand_samples = RandomSamples(model, test_loader, 6, num_samples=64, classes=10, save_r_scores='last')\n",
    "    R = rand_samples.artificial_step()\n",
    "\n",
    "    X, labels, const_model = rand_samples.X, rand_samples.lbls, rand_samples.const_model\n",
    "    device = torch.device('cuda') \n",
    "    model = model.to(device)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleDNNTrainerOld():\n",
    "\n",
    "    def __init__(self, model, X, labels, lr=.01):\n",
    "\n",
    "        self.model = model\n",
    "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(self.dev)\n",
    "\n",
    "        self.loss_red = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # self.scaler = GradScaler(enabled=True)\n",
    "\n",
    "        self.X, self.labels = X, labels\n",
    "\n",
    "    def apply_loss(self, y_pred, y):\n",
    "\n",
    "        y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
    "        y = y.reshape((-1,))\n",
    "\n",
    "        if y[0].dtype == torch.int: y = y.long()\n",
    "\n",
    "        return self.loss_red(y_pred, y)\n",
    "\n",
    "    def train_on_batch(self, batch):\n",
    "\n",
    "        self.model_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "\n",
    "        # Get data from batch, calculate loss and update\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images = images.to(self.device); labels = labels.to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        with autocast(device_type=self.dev):  # Enable mixed-precision\n",
    "\n",
    "            output = self.model(images)\n",
    "            l = self.apply_loss(output, labels)\n",
    "\n",
    "        self.scaler.scale(l).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        self.upd_model_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "\n",
    "        R = self.update_R()   \n",
    "        self.model.load_state_dict(self.upd_model_state)   \n",
    "\n",
    "        return R\n",
    "\n",
    "    def validate_on_batch(self, batch):\n",
    "\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(images)\n",
    "\n",
    "        acc = accuracy(output, labels, averaged=False)\n",
    "        return acc\n",
    "    \n",
    "    def update_R(self):\n",
    "        \n",
    "        causal = Causal(self.model, self.model_state, self.X, self.labels)\n",
    "        return causal.update()\n",
    "    \n",
    "def update(model, model_state, X, labels):\n",
    "    \n",
    "    causal = Causal(model, model_state, X, labels)\n",
    "    return causal.update()\n",
    "\n",
    "def check_differences(model, R, X, labels, lr=0.1, epochs=5):\n",
    "\n",
    "    # Apply two steps for the trainer, get models\n",
    "    trainer = simpleDNNTrainerOld(model, X, labels, lr)\n",
    "\n",
    "    R1 = R.clone()\n",
    "    R2 = R.clone()\n",
    "    R3 = R.clone()\n",
    "\n",
    "    start_model = {k: v.detach().clone() for k, v in trainer.model.state_dict().items()}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_bar = tqdm(enumerate(train_loader), \n",
    "                        desc=f\"Epoch: {epoch+1}/{epochs}\",\n",
    "                        total=len(train_loader)\n",
    "                    )\n",
    "        model_before = {k: v.detach().clone() for k, v in trainer.model.state_dict().items()}\n",
    "\n",
    "        for batch in train_bar:\n",
    "            \n",
    "            R_upd = trainer.train_on_batch(batch[1])\n",
    "            R1 += R_upd\n",
    "\n",
    "        R2 += update(trainer.model, model_before, X, labels)\n",
    "        trainer.model.load_state_dict(trainer.upd_model_state)\n",
    "\n",
    "        test_bar = tqdm(enumerate(test_loader), \n",
    "                        desc=f\"Epoch: {epoch+1}/{epochs}\",\n",
    "                        total=len(test_loader)\n",
    "                    )\n",
    "\n",
    "        metrics = np.zeros(len(test_loader.dataset))\n",
    "        batch_size = 0\n",
    "        \n",
    "        for batch in test_bar:\n",
    "            l = trainer.validate_on_batch(batch[1])\n",
    "\n",
    "            bs = batch[1][0].size(0)\n",
    "            metrics[batch_size: batch_size+bs] = l.cpu().numpy()\n",
    "            batch_size += batch[1][0].size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Accuracy: {np.mean(metrics)}\") \n",
    "\n",
    "    R3 = update(trainer.model, start_model, X, labels)\n",
    "\n",
    "    return R1, R2, R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9756954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deletion_AUC(model, X, labels, R1, R2, R3, k):\n",
    "\n",
    "    model.eval()\n",
    "    results = {\"compl\": [], \"skip_epoch\": [], \"skip_all\": []}\n",
    "    \n",
    "    for j, mth in enumerate([R1, R2, R3]):\n",
    "        for i, x in enumerate(X):  \n",
    "\n",
    "            method = \"compl\" if j==0 else \"skip_epoch\" if j==1 else \"skip_all\" \n",
    "            auc_value, _ = deletion_auc(model, x.to('cuda'), mth[i, :].to('cuda'), \n",
    "                                        labels[i], baseline_val=config['baseline_val'], steps=200)\n",
    "\n",
    "            try:\n",
    "                results[method].append(auc_value)\n",
    "            except Exception as e:\n",
    "                results[method].append(np.nan)\n",
    "            \n",
    "    res_pth = os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{k}\"\n",
    "    os.makedirs(f\"{res_pth}\", exist_ok=True)\n",
    "    with open(f\"{res_pth}/auc_compare.txt\", \"a+\") as f:\n",
    "\n",
    "        for method, auc_list in results.items():\n",
    "            avg_auc = np.nanmean(auc_list)\n",
    "            f.write(f\"Average Deletion AUC for {method:20s}: {avg_auc:.4f} \\n\")\n",
    "\n",
    "def calc_avg_drop(model, X, labels, R1, R2, R3, k):\n",
    "\n",
    "    model.eval()\n",
    "    results = {\"compl\": [], \"skip_epoch\": [], \"skip_all\": []}\n",
    "\n",
    "    for j, r in enumerate([R1, R2, R3]):\n",
    "\n",
    "        method = \"compl\" if j==0 else \"skip_epoch\" if j==1 else \"skip_all\" \n",
    "\n",
    "        score = evaluate_pixel_erasure(model, X, labels, r)\n",
    "        results[method] = score\n",
    "    \n",
    "    res_pth = os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{k}\"\n",
    "    \n",
    "    os.makedirs(f\"{res_pth}\", exist_ok=True)\n",
    "    with open(f\"{res_pth}/ad_compare.txt\", \"a+\") as f:\n",
    "\n",
    "        for method, ad_score in results.items():\n",
    "            f.write(f\"Average Drop for {method:20s}: {ad_score:.4f} \\n\")\n",
    "\n",
    "def plot(X, samples, R1, R2, R3, k):\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        for j, x in enumerate([X[samples[i], :], R1[samples[i], :], R2[samples[i], :]]): #, R3[samples[i], :]]):\n",
    "\n",
    "            title = f'sample_{i}' if j==0 else f'complete_{i}' if j==1 else f'skip_ep_{i}' if j==2 else f'skip_all_{i}'\n",
    "            plt.imshow(x.reshape(28, 28)); plt.axis('off')\n",
    "            plt.savefig(os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{k}/{title}.pdf\", \n",
    "                            bbox_inches='tight', format='pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2: 100%|██████████| 374/374 [00:04<00:00, 87.95it/s]\n",
      "Epoch: 1/2: 100%|██████████| 94/94 [00:00<00:00, 127.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 0.2834448160535117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/2: 100%|██████████| 374/374 [00:04<00:00, 88.20it/s]\n",
      "Epoch: 2/2: 100%|██████████| 94/94 [00:00<00:00, 126.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Accuracy: 0.3933110367892977\n"
     ]
    }
   ],
   "source": [
    "def run(k, epochs=10):\n",
    "\n",
    "    model = init_model().to(device).train()\n",
    "    lr = 0.05\n",
    "    R1, R2, R3 = check_differences(model, R, X, labels, lr=lr, epochs=epochs)\n",
    "\n",
    "    calc_deletion_AUC(model, X, labels, R1, R2, R3, k)\n",
    "    _ = benchmark_on_batch(model, X, labels, \n",
    "                        ['GS', 'IG'], 0, \n",
    "                        os.getcwd()+f\"/Results/Experiments/02_breaking_linearity/run_{k}\",\n",
    "                        0.6)\n",
    "\n",
    "    calc_avg_drop(model, X, labels, R1, R2, R3, k)\n",
    "\n",
    "    with open(os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{k}/setup.txt\", \"w\") as f:\n",
    "        f.write(f\"Learning Rate: {lr}\")\n",
    "\n",
    "    if k==10:\n",
    "        plot(X, range(20), R1, R2, R3, k)\n",
    "\n",
    "# Calculate mean scores for Deletion AUC\n",
    "for k in range(10):\n",
    "    run(k)\n",
    "\n",
    "# Test resulting attribution maps for one epoch\n",
    "run(10, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_auc, xep_auc, xl_auc = [], [], []\n",
    "xt_ad, xep_ad, xl_ad = [], [], []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    file = os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{i}/auc_compare.txt\"\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        lines = [l for l in f.readlines()]\n",
    "\n",
    "        xt_auc.append(float(lines[0].rsplit(\":\")[1][1:]))\n",
    "        xep_auc.append(float(lines[1].rsplit(\":\")[1][1:]))\n",
    "        xl_auc.append(float(lines[2].rsplit(\":\")[1][1:]))\n",
    "\n",
    "    file = os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/run_{i}/ad_compare.txt\"\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        lines = [l for l in f.readlines()]\n",
    "\n",
    "        xt_ad.append(float(lines[0].rsplit(\":\")[1][1:]))\n",
    "        xep_ad.append(float(lines[1].rsplit(\":\")[1][1:]))\n",
    "        xl_ad.append(float(lines[2].rsplit(\":\")[1][1:]))\n",
    "\n",
    "with open(os.getcwd() + \"/Results/Experiments/02_breaking_linearity/results.txt\", \"w+\") as f:\n",
    "\n",
    "    f.write('------------------ Deletion AUC ------------------ \\n\\n')\n",
    "    f.write(f\"Average Deletion AUC for X_train: {np.mean(xt_auc)} \\n\")\n",
    "    f.write(f\"Std for Deletion AUC for X_train: {np.std(xt_auc, ddof=1)} \\n\\n\")\n",
    "    f.write(f\"Average Deletion AUC for X_steps: {np.mean(xep_auc)} \\n\")\n",
    "    f.write(f\"Std for Deletion AUC for X_steps: {np.std(xep_auc, ddof=1)} \\n\\n\")\n",
    "    f.write(f\"Average Deletion AUC for X_linear: {np.mean(xl_auc)} \\n\")\n",
    "    f.write(f\"Std for Deletion AUC for X_linear: {np.std(xl_auc, ddof=1)} \\n\\n\")\n",
    "\n",
    "    f.write('------------------ Average_drop ------------------ \\n\\n')\n",
    "    f.write(f\"Average Drop for X_train: {np.mean(xt_ad)} \\n\")\n",
    "    f.write(f\"Average Drop for X_steps: {np.mean(xep_ad)} \\n\")\n",
    "    f.write(f\"Average Drop for X_linear': {np.mean(xl_ad)} \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
