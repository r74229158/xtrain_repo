{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc426a1",
   "metadata": {},
   "source": [
    "Consider moving notebook to the main directory to avoid path errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "from Training.Data.Modules.custom_loader import CustomLoader \n",
    "from Utils.model_utils import *; \n",
    "from Utils.configs import conf\n",
    "from XAI_Method.rand_samples import RandomSamples\n",
    "from XAI_Method.causal_effect_effic import Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a948b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    os.makedirs(os.getcwd() + f\"/Results/Experiments/02_breaking_linearity\", exist_ok=True)\n",
    "    \n",
    "    config = conf['simpleDNNTMnist']\n",
    "    def init_model(rand=True):\n",
    "        return get_model_architecture(config['model_name'], config['model_layers'] + [config[\"num_classes\"]], rand)\n",
    "\n",
    "    model = init_model()   \n",
    "    data_loader = CustomLoader(config[\"dataset\"], True, config[\"batch_size\"], shuffle_test=True)\n",
    "    train_loader, test_loader = data_loader.train_load, data_loader.evalu_load\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler(enabled=True)\n",
    "\n",
    "    rand_samples = RandomSamples(model, test_loader, 6, num_samples=64, classes=10, save_r_scores='last')\n",
    "    R = rand_samples.artificial_step()\n",
    "\n",
    "    X, labels, const_model = rand_samples.X, rand_samples.lbls, rand_samples.const_model\n",
    "    device = torch.device('cuda') \n",
    "    model = model.to(device)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6537da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleDNNTrainerOld():\n",
    "\n",
    "    def __init__(self, model, X, labels):\n",
    "\n",
    "        self.model = model\n",
    "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(self.dev)\n",
    "\n",
    "        self.loss_red = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "        self.scaler = GradScaler(enabled=True)\n",
    "\n",
    "        self.X, self.labels = X, labels\n",
    "\n",
    "    def apply_loss(self, y_pred, y):\n",
    "\n",
    "        y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
    "        y = y.reshape((-1,))\n",
    "\n",
    "        if y[0].dtype == torch.int: y = y.long()\n",
    "\n",
    "        return self.loss_red(y_pred, y)\n",
    "\n",
    "    def train_on_batch(self, batch):\n",
    "\n",
    "        self.model_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "\n",
    "        # Get data from batch, calculate loss and update\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images = images.to(self.device); labels = labels.to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        with autocast(device_type=self.dev):  # Enable mixed-precision\n",
    "\n",
    "            output = self.model(images)\n",
    "            l = self.apply_loss(output, labels)\n",
    "\n",
    "        self.scaler.scale(l).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        self.upd_model_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "\n",
    "        R = self.update_R()   \n",
    "        self.model.load_state_dict(self.upd_model_state)   \n",
    "\n",
    "        return self.model, R\n",
    "    \n",
    "    def update_R(self):\n",
    "        \n",
    "        causal = Causal(self.model, self.model_state, self.X, self.labels)\n",
    "        return causal.update()\n",
    "    \n",
    "def update(model, model_state, X, labels):\n",
    "    \n",
    "    causal = Causal(model, model_state, X, labels)\n",
    "    return causal.update()\n",
    "\n",
    "def check_differences(model, R, X, labels, break_pt=2):\n",
    "\n",
    "    # Apply two steps for the trainer, get models\n",
    "    trainer = simpleDNNTrainerOld(model, X, labels)\n",
    "\n",
    "    # First run\n",
    "    model_non_upd_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "    R1 = R.clone()\n",
    "\n",
    "    i = 0\n",
    "    for batch in train_loader:\n",
    "\n",
    "        model, R_upd = trainer.train_on_batch(batch)\n",
    "        R1 += R_upd\n",
    "\n",
    "        i += 1\n",
    "        if i == break_pt: break\n",
    "\n",
    "    # Second run\n",
    "    R2 = update(model, model_non_upd_state, X, labels)\n",
    "\n",
    "    return R1, R2\n",
    "\n",
    "def plot(X, samples, R1, R2):\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        for j, x in enumerate([X[samples[i], :], R1[samples[i], :], R2[samples[i], :]]):\n",
    "\n",
    "            tit = f'sample_{i}' if j==0 else f'non_linear_{i}' if j==1 else f'linear_{i}'\n",
    "            plt.imshow(x.reshape(28, 28)); plt.axis('off')\n",
    "            plt.savefig(os.getcwd() + f\"/Results/Experiments/02_breaking_linearity/{tit}.pdf\", \n",
    "                            bbox_inches='tight', format='pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0c3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1, R2 = check_differences(model, R, X, labels, break_pt=100)\n",
    "plot(X, [1, 3, 4, 6], R1, R2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
